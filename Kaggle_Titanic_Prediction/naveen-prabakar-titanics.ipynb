{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\n#Import the data (the training and test data)\n\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:01.817930Z","iopub.execute_input":"2025-02-08T17:50:01.818329Z","iopub.status.idle":"2025-02-08T17:50:03.001295Z","shell.execute_reply.started":"2025-02-08T17:50:01.818286Z","shell.execute_reply":"2025-02-08T17:50:02.999994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#clean the two-datasets (there are some missing values (for example cabin, age ))\n\n#fill in missing age with the median age\ntrain_df['Age'] = train_df['Age'].fillna(train_df['Age'].median())\ntest_df['Age'] = test_df['Age'].fillna(test_df['Age'].median())\n\n#fill in Embarked with the most frequent value\ntrain_df['Embarked'] = train_df['Embarked'].fillna(train_df['Embarked'].mode()[0])\ntest_df['Embarked'] = test_df['Embarked'].fillna(test_df['Embarked'].mode()[0])\n\n# Drop the Cabin column because it has too many missing values\ntrain_df.drop(columns=['Cabin'], inplace=True)\ntest_df.drop(columns=['Cabin'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:03.002621Z","iopub.execute_input":"2025-02-08T17:50:03.003037Z","iopub.status.idle":"2025-02-08T17:50:03.030122Z","shell.execute_reply.started":"2025-02-08T17:50:03.002994Z","shell.execute_reply":"2025-02-08T17:50:03.028829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Convert categories to numerical values before training the ML models\n\ntrain_df['Embarked'] = train_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\ntest_df['Embarked'] = test_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n\ntrain_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\ntest_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:03.031411Z","iopub.execute_input":"2025-02-08T17:50:03.031786Z","iopub.status.idle":"2025-02-08T17:50:03.043334Z","shell.execute_reply.started":"2025-02-08T17:50:03.031739Z","shell.execute_reply":"2025-02-08T17:50:03.041856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Other extra things added/dropped\n\n#added FamilySize to factor in the survivals\ntrain_df['FamilySize'] = train_df['SibSp'] + train_df['Parch']\ntest_df['FamilySize'] = test_df['SibSp'] + test_df['Parch']\n\n#dropped tickets, name, passengerID (they don't provide value to the model)\n\ntrain_df.drop(columns=['Name', 'Ticket', 'PassengerId'], inplace=True)\ntest_df.drop(columns=['Name', 'Ticket'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:03.046831Z","iopub.execute_input":"2025-02-08T17:50:03.047336Z","iopub.status.idle":"2025-02-08T17:50:03.063880Z","shell.execute_reply.started":"2025-02-08T17:50:03.047291Z","shell.execute_reply":"2025-02-08T17:50:03.062627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Random Forest model\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Separate features (X) and target (y)\nX = train_df.drop(columns=['Survived'])\ny = train_df['Survived']\n\n# Split the data into training and validation sets (80% train, 20% validation)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model (In this case the RandomForest model)\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the validation set\ny_pred = model.predict(X_val)\n\naccuracy = accuracy_score(y_val, y_pred)\n\n# Evaluate the accuracy\nprint(f'Accuracy: {accuracy:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:03.066128Z","iopub.execute_input":"2025-02-08T17:50:03.066587Z","iopub.status.idle":"2025-02-08T17:50:05.105601Z","shell.execute_reply.started":"2025-02-08T17:50:03.066542Z","shell.execute_reply":"2025-02-08T17:50:05.104043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Initialize the Logistic Regression model\nlogreg_model = LogisticRegression(max_iter=200, random_state=42)\n\n# Train the model\nlogreg_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred_logreg = logreg_model.predict(X_val)\n\n# Evaluate the accuracy\naccuracy_logreg = accuracy_score(y_val, y_pred_logreg)\nprint(f'Logistic Regression Accuracy: {accuracy_logreg:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:05.106870Z","iopub.execute_input":"2025-02-08T17:50:05.107577Z","iopub.status.idle":"2025-02-08T17:50:05.163825Z","shell.execute_reply.started":"2025-02-08T17:50:05.107517Z","shell.execute_reply":"2025-02-08T17:50:05.162505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Feature Engineering\ndef feature_engineering(df):\n    # Age Group\n    df['Age_Group'] = pd.cut(df['Age'], bins=[0, 12, 18, 30, 50, 80, 100], labels=['Child', 'Teen', 'Young_Adult', 'Adult', 'Middle_Aged', 'Senior'])\n    df['Age_Group'] = df['Age_Group'].astype('category').cat.codes\n    \n    # Fare Group\n    df['Fare_Group'] = pd.qcut(df['Fare'], 4, labels=['Low', 'Medium', 'High', 'Very_High'])\n    df['Fare_Group'] = df['Fare_Group'].astype('category').cat.codes\n    \n    # Family Size and Is_Alone\n    df['Family_Size'] = df['SibSp'] + df['Parch']\n    df['Is_Alone'] = df['Family_Size'].apply(lambda x: 1 if x == 0 else 0)\n    \n    return df\n\n# Apply feature engineering to both train and test sets\ntrain_df = feature_engineering(train_df)\ntest_df = feature_engineering(test_df)\n\n# Prepare the features and target\nX = train_df.drop(columns=['Survived'])\ny = train_df['Survived']\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_val)\n\n# Evaluate the accuracy\naccuracy = accuracy_score(y_val, y_pred)\nprint(f'Accuracy after feature engineering: {accuracy:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:05.165077Z","iopub.execute_input":"2025-02-08T17:50:05.165443Z","iopub.status.idle":"2025-02-08T17:50:05.446570Z","shell.execute_reply.started":"2025-02-08T17:50:05.165412Z","shell.execute_reply":"2025-02-08T17:50:05.445165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Initialize Random Forest with best parameters (hyperparameters)\nbest_rf_model = RandomForestClassifier(\n    bootstrap=True, \n    max_depth=10, \n    max_features='auto', \n    min_samples_leaf=2, \n    min_samples_split=2, \n    n_estimators=200, \n    random_state=42\n)\n\n# Train the model\nbest_rf_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred_best_rf = best_rf_model.predict(X_val)\n\n# Evaluate accuracy\nbest_rf_accuracy = accuracy_score(y_val, y_pred_best_rf)\nprint(f'Random Forest Accuracy with Best Parameters: {best_rf_accuracy:.4f}')\n\nmodel = best_rf_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:05.447711Z","iopub.execute_input":"2025-02-08T17:50:05.448044Z","iopub.status.idle":"2025-02-08T17:50:05.895219Z","shell.execute_reply.started":"2025-02-08T17:50:05.448013Z","shell.execute_reply":"2025-02-08T17:50:05.894014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (trying a different appraoch with xgboost model from professor rec)\n!pip install shap xgboost scikit-learn pandas numpy matplotlib seaborn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:05.896400Z","iopub.execute_input":"2025-02-08T17:50:05.896846Z","iopub.status.idle":"2025-02-08T17:50:11.734117Z","shell.execute_reply.started":"2025-02-08T17:50:05.896794Z","shell.execute_reply":"2025-02-08T17:50:11.732678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport shap\nimport os\nimport xgboost as xgb\nfrom google.colab import files\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n# Preserve PassengerId for final predictions\ntest_passenger_ids = test_df['PassengerId']\n\n# Feature Engineering - Extract Titles\ntrain_df['Title'] = train_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_df['Title'] = test_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# Normalize rare titles\nrare_titles = ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\ntrain_df['Title'] = train_df['Title'].replace(rare_titles, 'Rare')\ntest_df['Title'] = test_df['Title'].replace(rare_titles, 'Rare')\n\n# Encode categorical features\nfor col in ['Sex', 'Embarked', 'Title']:\n    le = LabelEncoder()\n    train_df[col] = le.fit_transform(train_df[col].astype(str))\n    test_df[col] = le.transform(test_df[col].astype(str))\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:11.735514Z","iopub.execute_input":"2025-02-08T17:50:11.735995Z","iopub.status.idle":"2025-02-08T17:50:19.254091Z","shell.execute_reply.started":"2025-02-08T17:50:11.735941Z","shell.execute_reply":"2025-02-08T17:50:19.253193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill missing values\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace=True)\ntest_df['Age'].fillna(test_df['Age'].median(), inplace=True)\ntrain_df['Fare'].fillna(train_df['Fare'].median(), inplace=True)\ntest_df['Fare'].fillna(test_df['Fare'].median(), inplace=True)\ntest_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)\n\n\n# Create new features\ntrain_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\ntest_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1\n\ntrain_df['IsAlone'] = (train_df['FamilySize'] == 1).astype(int)\ntest_df['IsAlone'] = (test_df['FamilySize'] == 1).astype(int)\n\ntrain_df['Age*Pclass'] = train_df['Age'] * train_df['Pclass']\ntest_df['Age*Pclass'] = test_df['Age'] * test_df['Pclass']\n\n\n# Drop unnecessary columns\ntrain_df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\ntest_df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:19.255397Z","iopub.execute_input":"2025-02-08T17:50:19.255825Z","iopub.status.idle":"2025-02-08T17:50:19.278459Z","shell.execute_reply.started":"2025-02-08T17:50:19.255784Z","shell.execute_reply":"2025-02-08T17:50:19.277155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature selection using SHAP\nX = train_df.drop(columns=['Survived'])\ny = train_df['Survived']\n\nmodel = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\nmodel.fit(X, y)\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)\n\nshap_importance = np.abs(shap_values).mean(axis=0)\nselected_features = X.columns[np.argsort(shap_importance)[-8:]]  # Keep top 8 features\n\nX = X[selected_features]\ntest_df = test_df[selected_features]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:19.279893Z","iopub.execute_input":"2025-02-08T17:50:19.280347Z","iopub.status.idle":"2025-02-08T17:50:19.719318Z","shell.execute_reply.started":"2025-02-08T17:50:19.280302Z","shell.execute_reply":"2025-02-08T17:50:19.718285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train final model with manually selected hyperparameters\nmodel = xgb.XGBClassifier(\n    n_estimators=300,\n    max_depth=7,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    gamma=1,\n    min_child_weight=2,\n    random_state=42\n)\nmodel.fit(X, y)\n\n# Evaluate final model\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\ny_pred = model.predict(X_val)\n\naccuracy = accuracy_score(y_val, y_pred)\nprint(f'Final Model Accuracy: {accuracy:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:19.723502Z","iopub.execute_input":"2025-02-08T17:50:19.723828Z","iopub.status.idle":"2025-02-08T17:50:19.859423Z","shell.execute_reply.started":"2025-02-08T17:50:19.723798Z","shell.execute_reply":"2025-02-08T17:50:19.858261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Cross validation to see accuracy accross other models\n\nfrom sklearn.model_selection import cross_val_score\n\ncv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n\naverage_accuracy = np.mean(cv_scores)\nprint(f'Average cross-validated accuracy: {average_accuracy:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:19.860892Z","iopub.execute_input":"2025-02-08T17:50:19.861195Z","iopub.status.idle":"2025-02-08T17:50:20.458561Z","shell.execute_reply.started":"2025-02-08T17:50:19.861168Z","shell.execute_reply":"2025-02-08T17:50:20.457084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make final predictions on test dataset\ntest_predictions = model.predict(test_df)\n\n# Create a DataFrame with PassengerId and predicted Survived values\nsubmission = pd.DataFrame({\n    'PassengerId': test_passenger_ids,\n    'Survived': test_predictions\n})\n\n# Save as CSV file\nsubmission.to_csv('submission.csv', index=False)\n\n# Display first few rows of submission file\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:20.459818Z","iopub.execute_input":"2025-02-08T17:50:20.460370Z","iopub.status.idle":"2025-02-08T17:50:20.499591Z","shell.execute_reply.started":"2025-02-08T17:50:20.460315Z","shell.execute_reply":"2025-02-08T17:50:20.498052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Visulize the results and work so far\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot the distribution of the target variable 'Survived'\nplt.figure(figsize=(6, 4))\nsns.countplot(x='Survived', data=train_df)\nplt.title('Survival Distribution')\nplt.xlabel('Survived (0 = No, 1 = Yes)')\nplt.ylabel('Count')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:20.500759Z","iopub.execute_input":"2025-02-08T17:50:20.501323Z","iopub.status.idle":"2025-02-08T17:50:20.982525Z","shell.execute_reply.started":"2025-02-08T17:50:20.501279Z","shell.execute_reply":"2025-02-08T17:50:20.981254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the distribution of Age\nplt.figure(figsize=(6, 4))\nsns.histplot(train_df['Age'].dropna(), bins=30, kde=True)\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:20.983694Z","iopub.execute_input":"2025-02-08T17:50:20.984007Z","iopub.status.idle":"2025-02-08T17:50:21.276076Z","shell.execute_reply.started":"2025-02-08T17:50:20.983981Z","shell.execute_reply":"2025-02-08T17:50:21.274706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the distribution of Fare\nplt.figure(figsize=(6, 4))\nsns.histplot(train_df['Fare'], bins=30, kde=True)\nplt.title('Fare Distribution')\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:21.277377Z","iopub.execute_input":"2025-02-08T17:50:21.277806Z","iopub.status.idle":"2025-02-08T17:50:21.529805Z","shell.execute_reply.started":"2025-02-08T17:50:21.277764Z","shell.execute_reply":"2025-02-08T17:50:21.528552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Embarked vs Survival\nplt.figure(figsize=(6, 4))\nsns.countplot(x='Embarked', hue='Survived', data=train_df)\nplt.title('Survival by Embarked')\nplt.xlabel('Embarked')\nplt.ylabel('Count')\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:21.530997Z","iopub.execute_input":"2025-02-08T17:50:21.531431Z","iopub.status.idle":"2025-02-08T17:50:21.751396Z","shell.execute_reply.started":"2025-02-08T17:50:21.531389Z","shell.execute_reply":"2025-02-08T17:50:21.750360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Reflection:\n\n\"\"\"\nFor Lab one, being able to visualize data and use machine learning models was an exciting experience. I got to use\npandas and seaborn to create graphs, and use Random Forest model to predict survivors. I previously took ds 201, so\nthis wasn't to new to me, but it was still fun to practice feature engineering to improve my accuracy. For the future,\nI would have used a different ML model (logistic regression since it is used for classification). Using other technqiues\nlike one hot encoding and hyper parameter tuning, I would also like to do in the feature. I would also like to improve my\ncross validations to check for accuracy across all models. these are the improvments I would do in the next potential \nlab we do. \n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T17:50:21.752371Z","iopub.execute_input":"2025-02-08T17:50:21.752753Z","iopub.status.idle":"2025-02-08T17:50:21.759217Z","shell.execute_reply.started":"2025-02-08T17:50:21.752722Z","shell.execute_reply":"2025-02-08T17:50:21.757972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}