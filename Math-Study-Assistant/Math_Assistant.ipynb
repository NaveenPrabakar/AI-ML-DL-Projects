{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aylp4NoNKYy5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf pinecone openai tiktoken orjson python-dotenv google.generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aEmEFWALFuC",
        "outputId": "454f0da3-613c-4008-c17e-f330321a3472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.0.0)\n",
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.12/dist-packages (7.3.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.100.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.12/dist-packages (3.11.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: google.generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2025.8.3)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from pinecone) (1.7.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone) (4.14.1)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google.generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google.generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google.generativeai) (2.179.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google.generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google.generativeai) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google.generativeai) (1.26.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google.generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google.generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google.generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google.generativeai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: packaging<25.0,>=24.2 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google.generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google.generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google.generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google.generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google.generativeai) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, uuid, orjson\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from pypdf import PdfReader\n",
        "from openai import OpenAI\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "try:\n",
        "    import tiktoken\n",
        "    ENCODER = tiktoken.get_encoding(\"cl100k_base\")\n",
        "except Exception:\n",
        "    ENCODER = None\n"
      ],
      "metadata": {
        "id": "cGsctH88LGnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, uuid, orjson\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from pypdf import PdfReader\n",
        "from openai import OpenAI\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "try:\n",
        "    import tiktoken\n",
        "    ENCODER = tiktoken.get_encoding(\"cl100k_base\")\n",
        "except Exception:\n",
        "    ENCODER = None\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
        "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\", \"\")\n",
        "PINECONE_INDEX  = os.environ.get(\"PINECONE_INDEX\", \"math-index\")\n",
        "\n",
        "EMBED_MODEL = \"text-embedding-3-small\"\n",
        "GEN_MODEL   = \"gpt-3.5-turbo\"\n",
        "\n",
        "TOP_K = 3\n",
        "MIN_SIM = 0.3\n",
        "CHUNK_TOKENS = 400\n",
        "CHUNK_OVERLAP = 49\n",
        "DEFAULT_NAMESPACE = \"math_textbook\"\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "if PINECONE_INDEX not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=PINECONE_INDEX,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "index = pc.Index(PINECONE_INDEX)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def num_tokens(s: str) -> int:\n",
        "    if ENCODER:\n",
        "        return len(ENCODER.encode(s))\n",
        "    return max(1, len(s.split()) // 0.75)\n",
        "\n",
        "@dataclass\n",
        "class DocChunk:\n",
        "    id: str\n",
        "    text: str\n",
        "    page: int\n",
        "    meta: Dict[str, Any]\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# PDF → Chunks → Pinecone\n",
        "# -----------------------------\n",
        "def pdf_to_pages(path: str) -> List[Tuple[int, str]]:\n",
        "    reader = PdfReader(path)\n",
        "    pages = []\n",
        "    for i, p in enumerate(reader.pages, start=1):\n",
        "        txt = p.extract_text() or \"\"\n",
        "        txt = \"\\n\".join(line.strip() for line in txt.splitlines())\n",
        "        pages.append((i, txt))\n",
        "    return pages\n",
        "\n",
        "def chunk_text(text: str, page: int, tokens: int = CHUNK_TOKENS, overlap: int = CHUNK_OVERLAP) -> List[Tuple[str,int]]:\n",
        "    words, out, buf = text.split(), [], []\n",
        "    for w in words:\n",
        "        buf.append(w)\n",
        "        if num_tokens(\" \".join(buf)) >= tokens:\n",
        "            out.append((\" \".join(buf), page))\n",
        "            buf = buf[-overlap:]\n",
        "    if buf:\n",
        "        out.append((\" \".join(buf), page))\n",
        "    return out\n",
        "\n",
        "def ingest_pdf(path: str, source: str, namespace: str = DEFAULT_NAMESPACE, year: int = 2024) -> int:\n",
        "    pages = pdf_to_pages(path)\n",
        "    chunks: List[DocChunk] = []\n",
        "    for page, txt in pages:\n",
        "        for chunk_txt, p in chunk_text(txt, page):\n",
        "            cid = f\"{source}-{p}-{uuid.uuid4().hex[:8]}\"\n",
        "            chunks.append(DocChunk(\n",
        "                id=cid,\n",
        "                text=chunk_txt,\n",
        "                page=p,\n",
        "                meta={\"source\": source, \"page\": p, \"year\": year, \"namespace\": namespace}\n",
        "            ))\n",
        "    # Embed and upsert\n",
        "    texts = [c.text for c in chunks]\n",
        "    embeds = []\n",
        "    for i in range(0, len(texts), 128):\n",
        "        resp = client.embeddings.create(model=EMBED_MODEL, input=texts[i:i+128])\n",
        "        embeds.extend([d.embedding for d in resp.data])\n",
        "    upserts = [{\"id\":c.id,\"values\":vec,\"metadata\":{**c.meta,\"text\":c.text}} for c,vec in zip(chunks,embeds)]\n",
        "    for i in range(0,len(upserts),100):\n",
        "        index.upsert(vectors=upserts[i:i+100], namespace=namespace)\n",
        "    return len(chunks)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Retrieval + QA\n",
        "# -----------------------------\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a helpful **math tutor assistant**.\\n\"\n",
        "    \"Rules: use ONLY provided textbook sources, explain step by step, and cite like [Source, p.Page].\\n\"\n",
        ")\n",
        "DISCLAIMER = \"Educational use only. Always double-check solutions.\"\n",
        "\n",
        "def retrieve(query: str, namespace: str = DEFAULT_NAMESPACE, k: int = TOP_K) -> List[Dict[str, Any]]:\n",
        "    qvec = client.embeddings.create(model=EMBED_MODEL, input=[query]).data[0].embedding\n",
        "    res = index.query(vector=qvec, top_k=k, include_metadata=True, namespace=namespace)\n",
        "    hits = []\n",
        "    for m in res.matches or []:\n",
        "        if getattr(m,\"score\",1.0) >= MIN_SIM:\n",
        "            hits.append({**(m.metadata or {}), \"_score\":m.score, \"_id\":m.id})\n",
        "    return hits\n",
        "\n",
        "def build_messages(query: str, ctx: List[Dict[str, Any]]):\n",
        "    ctx_str = \"\\n---\\n\".join([f\"[{c['source']}, p.{c['page']}]\\n{c['text'][:1000]}\" for c in ctx])\n",
        "    user = f\"Q: {query}\\n\\nContext:\\n{ctx_str}\\n\\nProvide a clear step-by-step solution. Add Disclaimer line.\"\n",
        "    return [{\"role\":\"system\",\"content\":SYSTEM_PROMPT},{\"role\":\"user\",\"content\":user}]\n",
        "\n",
        "def answer_query(query: str, namespace: str = DEFAULT_NAMESPACE):\n",
        "    ctx = retrieve(query, namespace, TOP_K)\n",
        "    if not ctx:\n",
        "        return {\"answer\":\"Insufficient info in sources.\", \"citations\":[], \"disclaimer\":DISCLAIMER}\n",
        "    msgs = build_messages(query, ctx)\n",
        "    resp = client.chat.completions.create(model=GEN_MODEL,messages=msgs,temperature=0.2)\n",
        "    return {\"answer\":resp.choices[0].message.content.strip(), \"citations\":[{\"source\":c['source'],\"page\":c['page']} for c in ctx], \"disclaimer\":DISCLAIMER}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Fine-tune dataset builder\n",
        "# -----------------------------\n",
        "def build_ft_example(question: str, ctx: List[Dict[str, Any]]):\n",
        "    cites = \", \".join([f\"{c['source']}, p.{c['page']}\" for c in ctx[:3]])\n",
        "    answer = f\"Step-by-step solution: <your curated solution>\\n\\nCitations: [{cites}]\\nDisclaimer: {DISCLAIMER}\"\n",
        "    return {\"messages\":[{\"role\":\"system\",\"content\":SYSTEM_PROMPT},{\"role\":\"user\",\"content\":question},{\"role\":\"assistant\",\"content\":answer}]}\n",
        "\n",
        "def write_jsonl(records: List[Dict], path: str):\n",
        "    with open(path,\"wb\") as f:\n",
        "        for r in records:\n",
        "            f.write(orjson.dumps(r))\n",
        "            f.write(b\"\\n\")\n"
      ],
      "metadata": {
        "id": "RIQZpaA1LIun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, uuid, orjson\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "from pypdf import PdfReader\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import google.generativeai as genai\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "GEMINI_API_KEY   = os.environ.get(\"GEMINI_API_KEY\", \"\")\n",
        "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\", \"\")\n",
        "PINECONE_INDEX   = os.environ.get(\"PINECONE_INDEX\", \"math-index-gemini\")\n",
        "\n",
        "EMBED_MODEL = \"models/embedding-001\"\n",
        "GEN_MODEL   = \"gemini-1.5-flash\"\n",
        "\n",
        "TOP_K = 10            # fetch more chunks\n",
        "MIN_SIM = 0.1         # lower threshold for better recall\n",
        "CHUNK_TOKENS = 400\n",
        "CHUNK_OVERLAP = 49\n",
        "DEFAULT_NAMESPACE = \"harrison21\"\n",
        "\n",
        "MAX_CHUNK_TEXT = 2000  # chars per chunk before embedding\n",
        "PREVIEW_LEN = 500      # chars stored in metadata\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "if PINECONE_INDEX not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=PINECONE_INDEX,\n",
        "        dimension=768,  # Gemini embeddings are 768-d\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "index = pc.Index(PINECONE_INDEX)\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class DocChunk:\n",
        "    id: str\n",
        "    text: str\n",
        "    page: int\n",
        "    meta: Dict[str, Any]\n",
        "\n",
        "def num_tokens(s: str) -> int:\n",
        "    return max(1, len(s.split()) // 0.75)\n",
        "\n",
        "def pdf_to_pages(path: str) -> List[Tuple[int, str]]:\n",
        "    reader = PdfReader(path)\n",
        "    pages = []\n",
        "    for i, p in enumerate(reader.pages, start=1):\n",
        "        txt = p.extract_text() or \"\"\n",
        "        txt = \"\\n\".join(line.strip() for line in txt.splitlines())\n",
        "        pages.append((i, txt))\n",
        "    return pages\n",
        "\n",
        "def chunk_text(text: str, page: int, tokens: int = CHUNK_TOKENS, overlap: int = CHUNK_OVERLAP):\n",
        "    words, out, buf = text.split(), [], []\n",
        "    for w in words:\n",
        "        buf.append(w)\n",
        "        if num_tokens(\" \".join(buf)) >= tokens:\n",
        "            out.append((\" \".join(buf), page))\n",
        "            buf = buf[-overlap:]\n",
        "    if buf:\n",
        "        out.append((\" \".join(buf), page))\n",
        "    return out\n",
        "\n",
        "def safe_split_text(text, max_len=MAX_CHUNK_TEXT):\n",
        "    pieces, start = [], 0\n",
        "    while start < len(text):\n",
        "        end = min(start + max_len, len(text))\n",
        "        pieces.append(text[start:end])\n",
        "        start = end\n",
        "    return pieces\n",
        "\n",
        "def flatten_embedding(embed):\n",
        "    if isinstance(embed, list) and all(isinstance(x, (float,int)) for x in embed):\n",
        "        return [float(x) for x in embed]\n",
        "    elif isinstance(embed, list) and all(isinstance(x, list) for x in embed):\n",
        "        return [float(x) for x in embed[0]]\n",
        "    raise ValueError(\"Unexpected embedding shape\")\n",
        "\n",
        "# -----------------------------\n",
        "# PDF → Pinecone ingestion\n",
        "# -----------------------------\n",
        "def ingest_pdf_safe(path: str, source: str, namespace: str = DEFAULT_NAMESPACE, year: int = 2024):\n",
        "    pages = pdf_to_pages(path)\n",
        "    chunks: List[DocChunk] = []\n",
        "\n",
        "    for page, txt in pages:\n",
        "        for chunk_txt, p in chunk_text(txt, page, tokens=CHUNK_TOKENS, overlap=CHUNK_OVERLAP):\n",
        "            for piece in safe_split_text(chunk_txt, MAX_CHUNK_TEXT):\n",
        "                cid = f\"{source}-{p}-{uuid.uuid4().hex[:8]}\"\n",
        "                chunks.append(DocChunk(\n",
        "                    id=cid,\n",
        "                    text=piece,\n",
        "                    page=p,\n",
        "                    meta={\"source\": source, \"page\": p, \"year\": year, \"namespace\": namespace}\n",
        "                ))\n",
        "\n",
        "    # embed in batches\n",
        "    upserts = []\n",
        "    for i in range(0, len(chunks), 128):\n",
        "        batch = chunks[i:i+128]\n",
        "        texts = [c.text for c in batch]\n",
        "\n",
        "        resp = genai.embed_content(model=EMBED_MODEL, content=texts)\n",
        "        embeddings = []\n",
        "        if isinstance(resp, dict) and \"embedding\" in resp:\n",
        "            embeddings.append(flatten_embedding(resp[\"embedding\"]))\n",
        "        elif isinstance(resp, list):\n",
        "            for r in resp:\n",
        "                embeddings.append(flatten_embedding(r.get(\"embedding\")))\n",
        "\n",
        "        for c, vec in zip(batch, embeddings):\n",
        "            if len(vec) != 768:\n",
        "                raise ValueError(f\"Embedding dimension mismatch: {len(vec)} != 768\")\n",
        "            upserts.append({\n",
        "                \"id\": c.id,\n",
        "                \"values\": vec,\n",
        "                \"metadata\": {**c.meta, \"text\": c.text[:PREVIEW_LEN]}\n",
        "            })\n",
        "\n",
        "    for i in range(0, len(upserts), 100):\n",
        "        index.upsert(vectors=upserts[i:i+100], namespace=namespace)\n",
        "\n",
        "    return len(chunks)\n",
        "\n",
        "# -----------------------------\n",
        "# Retrieval + QA\n",
        "# -----------------------------\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a helpful **math tutor assistant**.\\n\"\n",
        "    \"Rules: use provided textbook sources (not required), explain step by step, and cite like [Source, p.Page] if textbook is used.\\n\"\n",
        ")\n",
        "DISCLAIMER = \"Educational use only. Always double-check solutions.\"\n",
        "\n",
        "def retrieve(query: str, namespace: str = DEFAULT_NAMESPACE, k: int = TOP_K) -> List[Dict[str, Any]]:\n",
        "    resp = genai.embed_content(\n",
        "        model=EMBED_MODEL,\n",
        "        content=query,\n",
        "        task_type=\"RETRIEVAL_QUERY\",\n",
        "        output_dimensionality=768\n",
        "    )\n",
        "    qvec = flatten_embedding(resp[\"embedding\"])\n",
        "    res = index.query(vector=qvec, top_k=k, include_metadata=True, namespace=namespace)\n",
        "    hits = []\n",
        "    for m in res.matches or []:\n",
        "        if getattr(m, \"score\", 1.0) >= MIN_SIM:\n",
        "            hits.append({**(m.metadata or {}), \"_score\": m.score, \"_id\": m.id})\n",
        "    return hits\n",
        "\n",
        "def build_messages(query: str, ctx: List[Dict[str, Any]]):\n",
        "    ctx_str = \"\\n---\\n\".join([f\"[{c['source']}, p.{c['page']}]\\n{c['text'][:1000]}\" for c in ctx])\n",
        "    user = f\"Q: {query}\\n\\nContext:\\n{ctx_str}\\n\\nProvide a clear step-by-step solution. Add Disclaimer line.\"\n",
        "    return [{\"role\": \"user\", \"parts\": [SYSTEM_PROMPT + \"\\n\\n\" + user]}]\n",
        "\n",
        "def answer_query(query: str, namespace: str = DEFAULT_NAMESPACE):\n",
        "    ctx = retrieve(query, namespace, TOP_K)\n",
        "    if not ctx:\n",
        "        return {\"answer\": \"Insufficient info in sources.\", \"citations\": [], \"disclaimer\": DISCLAIMER}\n",
        "    msgs = build_messages(query, ctx)\n",
        "    model = genai.GenerativeModel(GEN_MODEL)\n",
        "    resp = model.generate_content(msgs)\n",
        "    return {\n",
        "        \"answer\": resp.text.strip(),\n",
        "        \"citations\": [{\"source\": c[\"source\"], \"page\": c[\"page\"]} for c in ctx],\n",
        "        \"disclaimer\": DISCLAIMER\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Fine-tune dataset builder\n",
        "# -----------------------------\n",
        "def build_ft_example(question: str, ctx: List[Dict[str, Any]]):\n",
        "    cites = \", \".join([f\"{c['source']}, p.{c['page']}\" for c in ctx[:3]])\n",
        "    answer = f\"Step-by-step solution: <your curated solution>\\n\\nCitations: [{cites}]\\nDisclaimer: {DISCLAIMER}\"\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "            {\"role\": \"assistant\", \"content\": answer}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "def write_jsonl(records: List[Dict], path: str):\n",
        "    with open(path, \"wb\") as f:\n",
        "        for r in records:\n",
        "            f.write(orjson.dumps(r))\n",
        "            f.write(b\"\\n\")"
      ],
      "metadata": {
        "id": "ap1o01DY-Iy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Ingest a PDF textbook:\n",
        "n = ingest_pdf_safe(\"/content/01. Introduction to Statistics Autor David M. Lane.pdf\", source=\"Harrison's (21e)\", namespace=\"harrison21\")\n",
        "print(\"Chunks indexed\", n)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cJKrBailLUKe",
        "outputId": "0930bde8-5a91-42e3-c9c9-fd9a86086397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks indexed 974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Ask a question:\n",
        "result = answer_query(\"\"\"Ages of students in a class:\n",
        "Given the data: 19, 21, 22, 20, 24, 22, 20, 19, 21\n",
        "a) Find the mean, median, and mode\n",
        "b) Compute the range, variance, and standard deviation\n",
        "\n",
        "A dataset has a mean of 75 and a standard deviation of 8.\n",
        "a) What score corresponds to a z-score of 1.5?\n",
        "b) Interpret the z-score in context.\"\"\", namespace=\"harrison21\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "P17Ym-f7LlId",
        "outputId": "eff44009-a095-4068-bfa5-44ceb461cc82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': \"**Disclaimer:** I am an AI chatbot and cannot provide financial, legal, or medical advice. The following is for educational purposes only.  I will do my best to answer your questions based on the provided context, but my knowledge is limited by the excerpts you've given.  I do not have access to the full textbook, so my calculations may be limited.\\n\\n\\n**Part 1: Ages of Students**\\n\\na) **Mean:**\\n\\n1. **Sum the ages:** 19 + 21 + 22 + 20 + 24 + 22 + 20 + 19 + 21 = 188\\n2. **Divide by the number of students:** 188 / 9 = 20.89 (approximately)\\n\\nTherefore, the mean age is approximately 20.89 years.\\n\\nb) **Median:**\\n\\n1. **Arrange the ages in ascending order:** 19, 19, 20, 20, 21, 21, 22, 22, 24\\n2. **Find the middle value:** Since there are 9 ages, the median is the 5th value.\\n\\nTherefore, the median age is 21 years.\\n\\nc) **Mode:**\\n\\n1. **Identify the most frequent age:** Both 20 and 22 appear twice.\\n\\nTherefore, the dataset is bimodal with modes of 20 and 22 years.\\n\\n\\nd) **Range:**\\n\\n1. **Subtract the lowest age from the highest age:** 24 - 19 = 5\\n\\nTherefore, the range of ages is 5 years.\\n\\n\\ne) **Variance:**\\n\\n1. **Calculate the deviations from the mean:**  We'll use the approximate mean of 20.89.  The deviations are: -1.89, 0.11, 1.11, -0.89, 3.11, 1.11, -0.89, -1.89, 0.11\\n2. **Square the deviations:** 3.5721, 0.0121, 1.2321, 0.7921, 9.6721, 1.2321, 0.7921, 3.5721, 0.0121\\n3. **Sum the squared deviations:** 20.887\\n4. **Divide by (n-1) for sample variance:** 20.887 / (9-1) = 2.61\\n\\nTherefore, the sample variance is approximately 2.61.\\n\\n\\nf) **Standard Deviation:**\\n\\n1. **Take the square root of the variance:** √2.61 ≈ 1.62\\n\\nTherefore, the sample standard deviation is approximately 1.62 years.\\n\\n\\n\\n**Part 2: Z-score**\\n\\na) **Score corresponding to z-score of 1.5:**\\n\\nThe formula for a z-score is:  z = (x - μ) / σ,  where:\\n\\n* z = z-score (1.5 in this case)\\n* x = the individual score we want to find\\n* μ = the mean (75)\\n* σ = the standard deviation (8)\\n\\nRearrange the formula to solve for x: x = zσ + μ\\n\\nx = (1.5 * 8) + 75 = 12 + 75 = 87\\n\\nTherefore, a z-score of 1.5 corresponds to a score of 87.\\n\\n\\nb) **Interpretation of the z-score:**\\n\\nA z-score of 1.5 indicates that the score of 87 is 1.5 standard deviations above the mean of 75.  This suggests the score is relatively high compared to the rest of the data.\\n\\n\\nThe provided textbook excerpts do not offer direct support for calculating the mean, median, mode, range, variance, and standard deviation, nor for calculating and interpreting z-scores.  These are standard statistical concepts and calculations.\", 'citations': [{'source': \"Harrison's (21e)\", 'page': 100.0}, {'source': \"Harrison's (21e)\", 'page': 363.0}, {'source': \"Harrison's (21e)\", 'page': 1.0}, {'source': \"Harrison's (21e)\", 'page': 462.0}, {'source': \"Harrison's (21e)\", 'page': 279.0}, {'source': \"Harrison's (21e)\", 'page': 639.0}, {'source': \"Harrison's (21e)\", 'page': 195.0}, {'source': \"Harrison's (21e)\", 'page': 556.0}], 'disclaimer': 'Educational use only. Always double-check solutions.'}\n"
          ]
        }
      ]
    }
  ]
}